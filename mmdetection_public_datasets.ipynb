{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMDetection Training on SageMaker with Public Datasets\n",
    "\n",
    "This notebook demonstrates how to train MMDetection models on Amazon SageMaker using public datasets. We'll cover:\n",
    "\n",
    "1. Building a custom training container\n",
    "2. Training with different public datasets (COCO sample, Balloon, Pascal VOC)\n",
    "3. Distributed training setup\n",
    "4. Model deployment\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- AWS CLI configured with appropriate permissions\n",
    "- Docker installed and running\n",
    "- SageMaker execution role with ECR and S3 permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Initialize SageMaker session\n",
    "session = sagemaker.Session()\n",
    "region = session.boto_region_name\n",
    "account = boto3.client('sts').get_caller_identity().get('Account')\n",
    "bucket = session.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Account: {account}\")\n",
    "print(f\"Bucket: {bucket}\")\n",
    "print(f\"Role: {role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build and Push Training Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Container configuration\n",
    "container_name = \"mmdetection-training\"\n",
    "tag = \"latest\"\n",
    "image_uri = f\"{account}.dkr.ecr.{region}.amazonaws.com/{container_name}:{tag}\"\n",
    "\n",
    "print(f\"Container image URI: {image_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to ECR\n",
    "!aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin 763104351884.dkr.ecr.{region}.amazonaws.com\n",
    "!aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin {account}.dkr.ecr.{region}.amazonaws.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ECR repository if it doesn't exist\n",
    "!aws ecr describe-repositories --repository-names {container_name} --region {region} || aws ecr create-repository --repository-name {container_name} --region {region}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and push the container\n",
    "!docker build -t {container_name}:{tag} -f Dockerfile.training .\n",
    "!docker tag {container_name}:{tag} {image_uri}\n",
    "!docker push {image_uri}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training Configuration\n",
    "\n",
    "### Available Public Datasets:\n",
    "- `coco_sample`: Small COCO dataset sample for quick testing\n",
    "- `balloon`: Balloon dataset from Mask R-CNN paper\n",
    "- `voc2007`: Pascal VOC 2007 dataset\n",
    "\n",
    "### Available Models:\n",
    "- Faster R-CNN\n",
    "- Mask R-CNN\n",
    "- RetinaNet\n",
    "- FCOS\n",
    "- YOLO series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "hyperparameters = {\n",
    "    'config-file': 'faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py',  # MMDetection config\n",
    "    'dataset': 'coco_sample',  # Dataset type\n",
    "    'download-dataset': True,  # Automatically download public dataset\n",
    "    'auto-scale': True,  # Auto-scale learning rate\n",
    "    'validate': True,  # Run validation during training\n",
    "    'options': 'train_cfg.max_epochs=5; default_hooks.checkpoint.interval=2'  # Custom options\n",
    "}\n",
    "\n",
    "print(\"Training hyperparameters:\")\n",
    "for key, value in hyperparameters.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Single Instance Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create estimator for single instance training\n",
    "estimator = Estimator(\n",
    "    image_uri=image_uri,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.g4dn.xlarge',  # GPU instance\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=f's3://{bucket}/mmdetection-output',\n",
    "    base_job_name='mmdetection-single',\n",
    "    use_spot_instances=True,  # Use spot instances to save cost\n",
    "    max_wait=7200,  # Maximum wait time for spot instances\n",
    "    max_run=3600,   # Maximum training time\n",
    ")\n",
    "\n",
    "print(\"Single instance estimator created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training (no input data needed as we're downloading public dataset)\n",
    "job_name = f\"mmdetection-single-{int(time.time())}\"\n",
    "estimator.fit(job_name=job_name)\n",
    "\n",
    "print(f\"Training job '{job_name}' started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Distributed Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update hyperparameters for distributed training\n",
    "distributed_hyperparameters = hyperparameters.copy()\n",
    "distributed_hyperparameters.update({\n",
    "    'config-file': 'mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py',  # Use Mask R-CNN for distributed training\n",
    "    'options': 'train_cfg.max_epochs=10; default_hooks.checkpoint.interval=5'  # Longer training\n",
    "})\n",
    "\n",
    "# Create distributed estimator\n",
    "distributed_estimator = Estimator(\n",
    "    image_uri=image_uri,\n",
    "    role=role,\n",
    "    instance_count=2,  # Use 2 instances\n",
    "    instance_type='ml.g4dn.xlarge',\n",
    "    hyperparameters=distributed_hyperparameters,\n",
    "    output_path=f's3://{bucket}/mmdetection-distributed-output',\n",
    "    base_job_name='mmdetection-distributed',\n",
    "    distribution={'pytorchddp': {'enabled': True}},  # Enable PyTorch DDP\n",
    "    use_spot_instances=True,\n",
    "    max_wait=7200,\n",
    "    max_run=5400,\n",
    ")\n",
    "\n",
    "print(\"Distributed estimator created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start distributed training\n",
    "distributed_job_name = f\"mmdetection-distributed-{int(time.time())}\"\n",
    "distributed_estimator.fit(job_name=distributed_job_name)\n",
    "\n",
    "print(f\"Distributed training job '{distributed_job_name}' started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training with Different Datasets and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example configurations for different models and datasets\n",
    "training_configs = [\n",
    "    {\n",
    "        'name': 'retinanet-balloon',\n",
    "        'config': {\n",
    "            'config-file': 'retinanet/retinanet_r50_fpn_1x_coco.py',\n",
    "            'dataset': 'balloon',\n",
    "            'download-dataset': True,\n",
    "            'options': 'train_cfg.max_epochs=20; default_hooks.checkpoint.interval=5'\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'fcos-voc2007',\n",
    "        'config': {\n",
    "            'config-file': 'fcos/fcos_r50_caffe_fpn_gn-head_1x_coco.py',\n",
    "            'dataset': 'voc2007',\n",
    "            'download-dataset': True,\n",
    "            'options': 'train_cfg.max_epochs=15; default_hooks.checkpoint.interval=3'\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'yolox-coco-sample',\n",
    "        'config': {\n",
    "            'config-file': 'yolox/yolox_s_8x8_300e_coco.py',\n",
    "            'dataset': 'coco_sample',\n",
    "            'download-dataset': True,\n",
    "            'options': 'train_cfg.max_epochs=50; default_hooks.checkpoint.interval=10'\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Available training configurations:\")\n",
    "for i, config in enumerate(training_configs):\n",
    "    print(f\"{i+1}. {config['name']}\")\n",
    "    for key, value in config['config'].items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run training with specific configuration\n",
    "def run_training_experiment(config_name, config_params, instance_type='ml.g4dn.xlarge'):\n",
    "    \"\"\"\n",
    "    Run a training experiment with specified configuration\n",
    "    \"\"\"\n",
    "    estimator = Estimator(\n",
    "        image_uri=image_uri,\n",
    "        role=role,\n",
    "        instance_count=1,\n",
    "        instance_type=instance_type,\n",
    "        hyperparameters=config_params,\n",
    "        output_path=f's3://{bucket}/mmdetection-experiments/{config_name}',\n",
    "        base_job_name=f'mmdet-{config_name}',\n",
    "        use_spot_instances=True,\n",
    "        max_wait=7200,\n",
    "        max_run=3600,\n",
    "    )\n",
    "    \n",
    "    job_name = f\"mmdet-{config_name}-{int(time.time())}\"\n",
    "    estimator.fit(job_name=job_name)\n",
    "    \n",
    "    return estimator, job_name\n",
    "\n",
    "# Example: Run RetinaNet training on balloon dataset\n",
    "# estimator, job_name = run_training_experiment(\n",
    "#     'retinanet-balloon', \n",
    "#     training_configs[0]['config']\n",
    "# )\n",
    "# print(f\"Started experiment: {job_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Monitor Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to monitor training job\n",
    "def monitor_training_job(job_name):\n",
    "    \"\"\"\n",
    "    Monitor training job status and logs\n",
    "    \"\"\"\n",
    "    sm_client = boto3.client('sagemaker')\n",
    "    \n",
    "    try:\n",
    "        response = sm_client.describe_training_job(TrainingJobName=job_name)\n",
    "        status = response['TrainingJobStatus']\n",
    "        \n",
    "        print(f\"Job Name: {job_name}\")\n",
    "        print(f\"Status: {status}\")\n",
    "        print(f\"Instance Type: {response['ResourceConfig']['InstanceType']}\")\n",
    "        print(f\"Instance Count: {response['ResourceConfig']['InstanceCount']}\")\n",
    "        \n",
    "        if 'TrainingStartTime' in response:\n",
    "            print(f\"Start Time: {response['TrainingStartTime']}\")\n",
    "        \n",
    "        if status == 'Completed':\n",
    "            print(f\"End Time: {response['TrainingEndTime']}\")\n",
    "            print(f\"Training Time: {response['TrainingTimeInSeconds']} seconds\")\n",
    "            print(f\"Model Artifacts: {response['ModelArtifacts']['S3ModelArtifacts']}\")\n",
    "        elif status == 'Failed':\n",
    "            print(f\"Failure Reason: {response.get('FailureReason', 'Unknown')}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error monitoring job: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "# monitor_training_job('your-job-name-here')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Deployment (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inference container (you would need to create this separately)\n",
    "# This is a placeholder for model deployment\n",
    "\n",
    "def deploy_model(estimator, instance_type='ml.m5.large'):\n",
    "    \"\"\"\n",
    "    Deploy trained model to SageMaker endpoint\n",
    "    Note: This requires a separate inference container\n",
    "    \"\"\"\n",
    "    try:\n",
    "        predictor = estimator.deploy(\n",
    "            initial_instance_count=1,\n",
    "            instance_type=instance_type,\n",
    "            endpoint_name=f'mmdetection-endpoint-{int(time.time())}'\n",
    "        )\n",
    "        return predictor\n",
    "    except Exception as e:\n",
    "        print(f\"Deployment failed: {e}\")\n",
    "        print(\"Note: You need to create a separate inference container for deployment\")\n",
    "        return None\n",
    "\n",
    "# Example:\n",
    "# predictor = deploy_model(estimator)\n",
    "# if predictor:\n",
    "#     print(f\"Model deployed to endpoint: {predictor.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cleanup Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to cleanup endpoints and models\n",
    "def cleanup_resources(endpoint_name=None):\n",
    "    \"\"\"\n",
    "    Clean up SageMaker resources\n",
    "    \"\"\"\n",
    "    sm_client = boto3.client('sagemaker')\n",
    "    \n",
    "    if endpoint_name:\n",
    "        try:\n",
    "            sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "            print(f\"Deleted endpoint: {endpoint_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting endpoint: {e}\")\n",
    "    \n",
    "    # List and optionally delete other resources\n",
    "    print(\"\\nActive endpoints:\")\n",
    "    try:\n",
    "        endpoints = sm_client.list_endpoints()['Endpoints']\n",
    "        for endpoint in endpoints:\n",
    "            if 'mmdetection' in endpoint['EndpointName'].lower():\n",
    "                print(f\"  - {endpoint['EndpointName']} ({endpoint['EndpointStatus']})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing endpoints: {e}\")\n",
    "\n",
    "# Example:\n",
    "# cleanup_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated how to:\n",
    "\n",
    "1. **Build a custom MMDetection training container** with support for multiple datasets\n",
    "2. **Train models using public datasets** without manual data preparation\n",
    "3. **Run both single-instance and distributed training**\n",
    "4. **Configure different model architectures** (Faster R-CNN, Mask R-CNN, RetinaNet, etc.)\n",
    "5. **Monitor training progress** and manage resources\n",
    "\n",
    "### Key Features:\n",
    "- **Automatic dataset download**: No need to manually prepare datasets\n",
    "- **Multiple dataset support**: COCO sample, Balloon, Pascal VOC 2007\n",
    "- **Flexible model configuration**: Easy to switch between different architectures\n",
    "- **Cost optimization**: Uses spot instances and configurable training duration\n",
    "- **Distributed training**: Supports multi-instance training for faster convergence\n",
    "\n",
    "### Next Steps:\n",
    "1. Create an inference container for model deployment\n",
    "2. Add support for custom datasets\n",
    "3. Implement hyperparameter tuning\n",
    "4. Add model evaluation and metrics tracking\n",
    "5. Set up MLOps pipeline for continuous training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
