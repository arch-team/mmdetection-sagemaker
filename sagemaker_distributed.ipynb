{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training MMDetection Mask-RCNN Model on Sagemaker Distributed Cluster\n",
    "\n",
    "## Motivation\n",
    "[MMDetection](https://github.com/open-mmlab/mmdetection) is a popular open-source Deep Learning framework focused on Computer Vision models and use cases. MMDetection provides to higher level APIs for model training and inference. It demonstrates [state-of-the-art benchmarks](https://github.com/open-mmlab/mmdetection#benchmark-and-model-zoo) for variety of model architecture and extensive Model Zoo.\n",
    "\n",
    "In this notebook, we will build a custom training container with MMdetection library and then train Mask-RCNN model from scratch on [COCO2017 dataset](https://cocodataset.org/#home) using Sagemaker distributed [training feature](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html) in order to reduce training time.\n",
    "\n",
    "### Preconditions\n",
    "- To execute this notebook, you will need to have COCO 2017 training and validation datasets uploaded to S3 bucket available for Amazon Sagemaker service.\n",
    "\n",
    "\n",
    "## Building Training Container\n",
    "\n",
    "Amazon Sagemaker allows to BYO containers for training, data processing, and inference. In our case, we need to build custom training container which will be pushed to your AWS account [ECR service](https://aws.amazon.com/ecr/). \n",
    "\n",
    "For this, we need to login to public ECR with Sagemaker base images and private ECR reposity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker, boto3\n",
    "\n",
    "session = sagemaker.Session()\n",
    "region = session.boto_region_name\n",
    "account = boto3.client('sts').get_caller_identity().get('Account')\n",
    "bucket = session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# login to Sagemaker ECR with Deep Learning Containers\n",
    "!aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin 763104351884.dkr.ecr.{region}.amazonaws.com\n",
    "# login to your private ECR\n",
    "!aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin {account}.dkr.ecr.{region}.amazonaws.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let review training container:\n",
    "- use Sagemaker PyTorch 1.5.0 container as base image;\n",
    "- install latest version of Pytorch libraries and MMdetection dependencies;\n",
    "- build MMDetection from sources;\n",
    "- configure Sagemaker env variables, specifically, what script to use at training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m# Use Sagemaker PyTorch container as base image\u001b[39;49;00m\n",
      "\u001b[37m# https://github.com/aws/sagemaker-pytorch-container/blob/master/docker/1.5.0/py3/Dockerfile.gpu\u001b[39;49;00m\n",
      "\u001b[34mFROM\u001b[39;49;00m \u001b[33m763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.5.0-gpu-py36-cu101-ubuntu16.04\u001b[39;49;00m\n",
      "\u001b[34mLABEL\u001b[39;49;00m \u001b[31mauthor\u001b[39;49;00m=\u001b[33m\"vadimd@amazon.com\"\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m############# Installing MMDetection from source ############\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mWORKDIR\u001b[39;49;00m\u001b[33m /opt/ml/code\u001b[39;49;00m\n",
      "\u001b[34mRUN\u001b[39;49;00m pip install --upgrade --force-reinstall  torch torchvision cython\n",
      "\u001b[34mRUN\u001b[39;49;00m pip install mmcv-full==latest+torch1.6.0+cu101 -f https://download.openmmlab.com/mmcv/dist/index.html\n",
      "\n",
      "\u001b[34mRUN\u001b[39;49;00m git clone https://github.com/open-mmlab/mmdetection\n",
      "\u001b[34mRUN\u001b[39;49;00m \u001b[36mcd\u001b[39;49;00m mmdetection/ && \u001b[33m\\\u001b[39;49;00m\n",
      "    pip install -e .\n",
      "\n",
      "\u001b[37m# to address https://github.com/pytorch/pytorch/issues/37377\u001b[39;49;00m\n",
      "\u001b[34mENV\u001b[39;49;00m MKL_THREADING_LAYER GNU\n",
      "\u001b[34mENV\u001b[39;49;00m MMDETECTION /opt/ml/code/mmdetection\n",
      "\n",
      "\u001b[37m############# Configuring Sagemaker ##############\u001b[39;49;00m\n",
      "\u001b[34mCOPY\u001b[39;49;00m container_training /opt/ml/code\n",
      "\n",
      "\u001b[34mENV\u001b[39;49;00m SAGEMAKER_SUBMIT_DIRECTORY /opt/ml/code\n",
      "\u001b[34mENV\u001b[39;49;00m SAGEMAKER_PROGRAM mmdetection_train.py\n",
      "\n",
      "\u001b[34mWORKDIR\u001b[39;49;00m\u001b[33m /\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "! pygmentize -l docker Dockerfile.training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "Next, we build and push custom training container to private ECR\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./build_and_push.sh mmdetection-training latest Dockerfile.training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training script\n",
    "\n",
    "At training time, Sagemaker executes training script defined in `SAGEMAKER_PROGRAM` variable. In our case, this script does following\n",
    "- parses user parameters passed via Sagemaker Hyperparameter dictionary;\n",
    "- based on parameters constructs launch command;\n",
    "- uses `torch.distributed.launch` utility to launch distributed training;\n",
    "- uses MMDetection `tools/train.py` to configure trianing process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ArgumentParser\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmmcv\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Config\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mshutil\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_training_world\u001b[39;49;00m():\n",
      "\n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    Calculates number of devices in Sagemaker distributed cluster\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "\n",
      "    \u001b[37m# Get params of Sagemaker distributed cluster from predefined env variables\u001b[39;49;00m\n",
      "    num_gpus = \u001b[36mint\u001b[39;49;00m(os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    num_cpus = \u001b[36mint\u001b[39;49;00m(os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_NUM_CPUS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    hosts = json.loads(os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    current_host = os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "\n",
      "    \u001b[37m# Define PyTorch training world\u001b[39;49;00m\n",
      "    world = {}\n",
      "    world[\u001b[33m\"\u001b[39;49;00m\u001b[33mnumber_of_processes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = num_gpus \u001b[34mif\u001b[39;49;00m num_gpus > \u001b[34m0\u001b[39;49;00m \u001b[34melse\u001b[39;49;00m num_cpus\n",
      "    world[\u001b[33m\"\u001b[39;49;00m\u001b[33mnumber_of_machines\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = \u001b[36mlen\u001b[39;49;00m(hosts)\n",
      "    world[\u001b[33m\"\u001b[39;49;00m\u001b[33msize\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = world[\u001b[33m\"\u001b[39;49;00m\u001b[33mnumber_of_processes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] * world[\u001b[33m\"\u001b[39;49;00m\u001b[33mnumber_of_machines\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "    world[\u001b[33m\"\u001b[39;49;00m\u001b[33mmachine_rank\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = hosts.index(current_host)\n",
      "    world[\u001b[33m\"\u001b[39;49;00m\u001b[33mmaster_addr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = hosts[\u001b[34m0\u001b[39;49;00m]\n",
      "    world[\u001b[33m\"\u001b[39;49;00m\u001b[33mmaster_port\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = \u001b[33m\"\u001b[39;49;00m\u001b[33m55555\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[37m# port is defined by Sagemaker\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m world\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtraining_configurator\u001b[39;49;00m(args, world):\n",
      "    \n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    Configure training process by updating config file: \u001b[39;49;00m\n",
      "\u001b[33m    - takes base config from MMDetection templates;\u001b[39;49;00m\n",
      "\u001b[33m    - updates it with SageMaker specific data locations;\u001b[39;49;00m\n",
      "\u001b[33m    - overrides with user-defined options.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \n",
      "    \u001b[37m# updating path to config file inside SM container\u001b[39;49;00m\n",
      "    abs_config_path = os.path.join(\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/code/mmdetection\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, args.config_file)\n",
      "    cfg = Config.fromfile(abs_config_path)\n",
      "    \n",
      "    \u001b[34mif\u001b[39;49;00m args.dataset.lower() == \u001b[33m\"\u001b[39;49;00m\u001b[33mcoco\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "        \n",
      "        cfg.data_root = os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAINING\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] \u001b[37m# By default, data will be download to /opt/ml/input/data/training\u001b[39;49;00m\n",
      "        cfg.data.train.ann_file = os.path.join(cfg.data_root, \u001b[33m\"\u001b[39;49;00m\u001b[33mannotations/instances_train2017.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        cfg.data.train.img_prefix = os.path.join(cfg.data_root, \u001b[33m\"\u001b[39;49;00m\u001b[33mtrain2017\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        cfg.data.val.ann_file = os.path.join(cfg.data_root, \u001b[33m\"\u001b[39;49;00m\u001b[33mannotations/instances_val2017.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        cfg.data.val.img_prefix = os.path.join(cfg.data_root, \u001b[33m\"\u001b[39;49;00m\u001b[33mval2017\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        \n",
      "        \u001b[37m# Note, that we are using validation dataset for testing purposes\u001b[39;49;00m\n",
      "        cfg.data.test.ann_file = os.path.join(cfg.data_root, \u001b[33m\"\u001b[39;49;00m\u001b[33mannotations/instances_val2017.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        cfg.data.test.img_prefix = os.path.join(cfg.data_root, \u001b[33m\"\u001b[39;49;00m\u001b[33mval2017\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        \n",
      "        \u001b[37m# Overriding config with options\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m args.options \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\n",
      "            cfg.merge_from_dict(args.options)\n",
      "        \n",
      "        \u001b[37m# scaling LR based on number of training processes\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m args.auto_scale:\n",
      "            cfg = auto_scale_config(cfg, world)\n",
      "        \n",
      "        updated_config = os.path.join(os.getcwd(), \u001b[33m\"\u001b[39;49;00m\u001b[33mupdated_config.py\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        cfg.dump(updated_config)\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mFollowing config will be used for training:\u001b[39;49;00m\u001b[33m{cfg.pretty_text}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        \n",
      "    \u001b[34melse\u001b[39;49;00m:\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mNotImplementedError\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mDataset \u001b[39;49;00m\u001b[33m{args.dataset}\u001b[39;49;00m\u001b[33m is not implemented.\u001b[39;49;00m\u001b[33m\\\u001b[39;49;00m\n",
      "\u001b[33m                                    Currently only COCO-style datasets are available.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "              \n",
      "    \u001b[34mreturn\u001b[39;49;00m updated_config\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mauto_scale_config\u001b[39;49;00m(cfg, world):\n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    Method automatically scales learning rate\u001b[39;49;00m\n",
      "\u001b[33m    based on number of processes in distributed cluster.\u001b[39;49;00m\n",
      "\u001b[33m    \u001b[39;49;00m\n",
      "\u001b[33m    When scaling, we take user-provided config as a config for single node with 8 GPUs\u001b[39;49;00m\n",
      "\u001b[33m    and scale it based on total number of training processes.\u001b[39;49;00m\n",
      "\u001b[33m    \u001b[39;49;00m\n",
      "\u001b[33m    Note, that batch size is not scaled, as MMDetection uses relative\u001b[39;49;00m\n",
      "\u001b[33m    batch size: cfg.data.samples_per_gpu\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \n",
      "    old_world_size = \u001b[34m8\u001b[39;49;00m \u001b[37m# Note, this is a hardcoded value, as MMDetection configs are build for single 8-GPU V100 node.\u001b[39;49;00m\n",
      "    old_lr = cfg.optimizer.lr\n",
      "    old_lr_warmup = cfg.lr_config.warmup_iters\n",
      "    scale = world[\u001b[33m\"\u001b[39;49;00m\u001b[33msize\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] / old_world_size\n",
      "    \n",
      "    cfg.optimizer.lr = old_lr * scale\n",
      "    cfg.lr_config.warmup_iters = old_lr_warmup / scale\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mInitial learning rate \u001b[39;49;00m\u001b[33m{old_lr}\u001b[39;49;00m\u001b[33m and warmup \u001b[39;49;00m\u001b[33m{old_lr_warmup}\u001b[39;49;00m\u001b[33m were scaled \u001b[39;49;00m\u001b[33m\\\u001b[39;49;00m\n",
      "\u001b[33m          to \u001b[39;49;00m\u001b[33m{cfg.optimizer.lr}\u001b[39;49;00m\u001b[33m and \u001b[39;49;00m\u001b[33m{cfg.lr_config.warmup_iters}\u001b[39;49;00m\u001b[33m respectively.\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m          Each GPU has batch size of \u001b[39;49;00m\u001b[33m{cfg.data.samples_per_gpu}\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m          Total number of GPUs in training cluster is \u001b[39;49;00m\u001b[33m{world['size']}\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m          Effective batch size is \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[33mcfg.data.samples_per_gpu * world[\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33msize\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m]}\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m cfg\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32moptions_to_dict\u001b[39;49;00m(options):\n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    Takes string of options in format of 'key1=value1; key2=value2 ...'\u001b[39;49;00m\n",
      "\u001b[33m    and produces dictionary object {'key1': 'value1', 'key2':'value2'...}.\u001b[39;49;00m\n",
      "\u001b[33m    \u001b[39;49;00m\n",
      "\u001b[33m    It also supports lists of values: key3=v1,v2,v3.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \n",
      "    options_dict = \u001b[36mdict\u001b[39;49;00m(item.split(\u001b[33m\"\u001b[39;49;00m\u001b[33m=\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mfor\u001b[39;49;00m item \u001b[35min\u001b[39;49;00m options.split(\u001b[33m\"\u001b[39;49;00m\u001b[33m; \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)) \n",
      "    \n",
      "    \u001b[34mfor\u001b[39;49;00m key, value \u001b[35min\u001b[39;49;00m options_dict.items():\n",
      "        value = [_parse_int_float_bool(v) \u001b[34mfor\u001b[39;49;00m v \u001b[35min\u001b[39;49;00m value.split(\u001b[33m\"\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)]\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(value) == \u001b[34m1\u001b[39;49;00m:\n",
      "            value = value[\u001b[34m0\u001b[39;49;00m]\n",
      "        options_dict[key] = value\n",
      "    \u001b[34mreturn\u001b[39;49;00m options_dict\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_parse_int_float_bool\u001b[39;49;00m(val):\n",
      "    \u001b[34mtry\u001b[39;49;00m:\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mint\u001b[39;49;00m(val)\n",
      "    \u001b[34mexcept\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m:\n",
      "        \u001b[34mpass\u001b[39;49;00m\n",
      "    \u001b[34mtry\u001b[39;49;00m:\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mfloat\u001b[39;49;00m(val)\n",
      "    \u001b[34mexcept\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m:\n",
      "        \u001b[34mpass\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m val.lower() \u001b[35min\u001b[39;49;00m [\u001b[33m'\u001b[39;49;00m\u001b[33mtrue\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mfalse\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]:\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[34mTrue\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m val.lower() == \u001b[33m'\u001b[39;49;00m\u001b[33mtrue\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[34melse\u001b[39;49;00m \u001b[34mFalse\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m val\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32msave_model\u001b[39;49;00m(config_path, work_dir, model_dir):\n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    This method copies model trained weights and config \u001b[39;49;00m\n",
      "\u001b[33m    from output directory to model directory.\u001b[39;49;00m\n",
      "\u001b[33m    Sagemaker then automatically archives content of model directory\u001b[39;49;00m\n",
      "\u001b[33m    and adds it to model registry once training job is completed.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \n",
      "\n",
      "    \u001b[37m# First copy config file\u001b[39;49;00m\n",
      "    \u001b[34mtry\u001b[39;49;00m:\n",
      "        new_config_path = os.path.join(model_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mconfig.py\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        shutil.copyfile(config_path, new_config_path)\n",
      "    \u001b[34mexcept\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m e:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mException when trying to copy \u001b[39;49;00m\u001b[33m{config_path}\u001b[39;49;00m\u001b[33m to \u001b[39;49;00m\u001b[33m{new_config_path}\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        \u001b[36mprint\u001b[39;49;00m(e)\n",
      "    \n",
      "    \n",
      "    \u001b[37m# Then copy checkpoints from work_dir\u001b[39;49;00m\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m os.listdir(work_dir):\n",
      "        \u001b[34mif\u001b[39;49;00m file.endswith(\u001b[33m\"\u001b[39;49;00m\u001b[33m.pth\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n",
      "            \u001b[34mtry\u001b[39;49;00m:\n",
      "                checkpoint_path = os.path.join(work_dir, file)\n",
      "                new_checkpoint_path = os.path.join(model_dir, file)\n",
      "                shutil.copyfile(checkpoint_path, new_checkpoint_path)\n",
      "            \u001b[34mexcept\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m e:\n",
      "                \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mException when trying to copy \u001b[39;49;00m\u001b[33m{checkpoint_path}\u001b[39;49;00m\u001b[33m to \u001b[39;49;00m\u001b[33m{new_checkpoint_path}\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "                \u001b[36mprint\u001b[39;49;00m(e)\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mModel config and checkpoints are saved to \u001b[39;49;00m\u001b[33m{model_dir}\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "    \n",
      "    \u001b[37m# Get initial configuration to select appropriate HuggingFace task and its configuration\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mStarting training...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser = ArgumentParser()\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--config-file\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[34mNone\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mFILE\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \n",
      "                        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mOnly default MMDetection configs are supported now. \u001b[39;49;00m\u001b[33m\\\u001b[39;49;00m\n",
      "\u001b[33m                        See for details: https://github.com/open-mmlab/mmdetection/tree/master/configs/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--dataset\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mcoco\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mDefine which dataset format to use.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--options\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, nargs=\u001b[33m'\u001b[39;49;00m\u001b[33m+\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[34mNone\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mConfig overrides.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--auto-scale\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[34mlambda\u001b[39;49;00m s: s.lower() \u001b[35min\u001b[39;49;00m [\u001b[33m'\u001b[39;49;00m\u001b[33mtrue\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33myes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], \n",
      "                        default=\u001b[34mFalse\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mwhether to scale batch parameters and learning rate based on cluster size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--validate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[34mlambda\u001b[39;49;00m s: s.lower() \u001b[35min\u001b[39;49;00m [\u001b[33m'\u001b[39;49;00m\u001b[33mtrue\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33myes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], \n",
      "                    default=\u001b[34mFalse\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mwhether to scale batch parameters and learning rate based on cluster size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \n",
      "    args, unknown = parser.parse_known_args()\n",
      "    \n",
      "    \u001b[34mif\u001b[39;49;00m args.options \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\n",
      "        args.options = options_to_dict(args.options[\u001b[34m0\u001b[39;49;00m])        \n",
      "    \n",
      "    \u001b[34mif\u001b[39;49;00m unknown:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mFollowing arguments were not recognized and won\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mt be used: \u001b[39;49;00m\u001b[33m{unknown}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Derive parameters of distributed training cluster in Sagemaker\u001b[39;49;00m\n",
      "    world = get_training_world()  \n",
      "\n",
      "    \u001b[37m# Update config file\u001b[39;49;00m\n",
      "    config_file = training_configurator(args, world)\n",
      "              \n",
      "    \u001b[37m# Train script config\u001b[39;49;00m\n",
      "    launch_config = [ \u001b[33m\"\u001b[39;49;00m\u001b[33mpython -m torch.distributed.launch\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \n",
      "                     \u001b[33m\"\u001b[39;49;00m\u001b[33m--nnodes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mstr\u001b[39;49;00m(world[\u001b[33m'\u001b[39;49;00m\u001b[33mnumber_of_machines\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]), \u001b[33m\"\u001b[39;49;00m\u001b[33m--node_rank\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mstr\u001b[39;49;00m(world[\u001b[33m'\u001b[39;49;00m\u001b[33mmachine_rank\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]),\n",
      "                     \u001b[33m\"\u001b[39;49;00m\u001b[33m--nproc_per_node\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mstr\u001b[39;49;00m(world[\u001b[33m'\u001b[39;49;00m\u001b[33mnumber_of_processes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]), \u001b[33m\"\u001b[39;49;00m\u001b[33m--master_addr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, world[\u001b[33m'\u001b[39;49;00m\u001b[33mmaster_addr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], \n",
      "                     \u001b[33m\"\u001b[39;49;00m\u001b[33m--master_port\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, world[\u001b[33m'\u001b[39;49;00m\u001b[33mmaster_port\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]]\n",
      " \n",
      "    train_config = [os.path.join(os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mMMDETECTION\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], \u001b[33m\"\u001b[39;49;00m\u001b[33mtools/train.py\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), \n",
      "                    config_file, \n",
      "                    \u001b[33m\"\u001b[39;49;00m\u001b[33m--launcher\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mpytorch\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \n",
      "                    \u001b[33m\"\u001b[39;49;00m\u001b[33m--work-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]]\n",
      "    \n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m args.validate:\n",
      "        train_config.append(\u001b[33m\"\u001b[39;49;00m\u001b[33m--no-validate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Concat Pytorch Distributed Launch config and MMdetection config\u001b[39;49;00m\n",
      "    joint_cmd = \u001b[33m\"\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join(\u001b[36mstr\u001b[39;49;00m(x) \u001b[34mfor\u001b[39;49;00m x \u001b[35min\u001b[39;49;00m launch_config+train_config)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mFollowing command will be executed: \u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, joint_cmd)\n",
      "    \n",
      "    process = subprocess.Popen(joint_cmd,  stderr=subprocess.STDOUT, stdout=subprocess.PIPE, shell=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[34mwhile\u001b[39;49;00m \u001b[34mTrue\u001b[39;49;00m:\n",
      "        output = process.stdout.readline()\n",
      "        \n",
      "        \u001b[34mif\u001b[39;49;00m process.poll() \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\n",
      "            \u001b[34mbreak\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m output:\n",
      "            \u001b[36mprint\u001b[39;49;00m(output.decode(\u001b[33m\"\u001b[39;49;00m\u001b[33mutf-8\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).strip())\n",
      "    rc = process.poll()\n",
      "    \n",
      "    \u001b[34mif\u001b[39;49;00m process.returncode != \u001b[34m0\u001b[39;49;00m:\n",
      "        \u001b[34mraise\u001b[39;49;00m subprocess.CalledProcessError(returncode=process.returncode, cmd=joint_cmd)\n",
      "    \n",
      "    \u001b[37m# Before completing training, saving model artifacts\u001b[39;49;00m\n",
      "    save_model(config_file, os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    \n",
      "    sys.exit(process.returncode)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "! pygmentize container_training/mmdetection_train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Sagemaker Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define IAM role\n",
    "import boto3\n",
    "import re\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "prefix_input = 'mmdetection-input'\n",
    "prefix_output = 'mmdetection-ouput'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = \"mmdetection-training\" # your container name\n",
    "tag = \"latest\"\n",
    "image = '{}.dkr.ecr.{}.amazonaws.com/{}:{}'.format(account, region, container, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithm parameters\n",
    "\n",
    "hyperparameters = {\n",
    "    \"config-file\" : \"configs/mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py\", # config path is relative to MMDetection root directory\n",
    "    \"dataset\" : \"coco\",\n",
    "    \"auto-scale\" : \"false\", # whether to scale LR and Warm Up time\n",
    "    \"validate\" : \"true\", # whether to run validation after training is done\n",
    "    \n",
    "    # 'options' allows to override individual config values\n",
    "    \"options\" : \"total_epochs=1; optimizer.lr=0.08; evaluation.gpu_collect=True\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sagemaker will parse metrics from STDOUT and store/visualize them as part of training job\n",
    "metrics = [\n",
    "    {\n",
    "        \"Name\": \"loss\",\n",
    "        \"Regex\": \".*loss:\\s([0-9\\\\.]+)\\s*\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"loss_rpn_cls\",\n",
    "        \"Regex\": \".*loss_rpn_cls:\\s([0-9\\\\.]+)\\s*\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"loss_rpn_bbox\",\n",
    "        \"Regex\": \".*loss_rpn_bbox:\\s([0-9\\\\.]+)\\s*\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"loss_cls\",\n",
    "        \"Regex\": \".*loss_cls:\\s([0-9\\\\.]+)\\s*\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"acc\",\n",
    "        \"Regex\": \".*acc:\\s([0-9\\\\.]+)\\s*\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"loss_bbox\",\n",
    "        \"Regex\": \".*loss_bbox:\\s([0-9\\\\.]+)\\s*\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"loss_mask\",\n",
    "        \"Regex\": \".*loss_mask:\\s([0-9\\\\.]+)\\s*\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"lr\",\n",
    "        \"Regex\": \"lr: (-?\\d+.?\\d*(?:[Ee]-\\d+)?)\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "Execute cell below to start training on Sagemaker.\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = sagemaker.estimator.Estimator(image,\n",
    "                                          role=role,\n",
    "                                          train_instance_count=4,\n",
    "                                          train_instance_type='ml.p3.16xlarge',\n",
    "                                          train_volume_size=100,\n",
    "                                          output_path=\"s3://{}/{}\".format(bucket, prefix_output),\n",
    "                                          metric_definitions = metrics,\n",
    "                                          hyperparameters = hyperparameters, \n",
    "                                          sagemaker_session=session\n",
    ")\n",
    "\n",
    "est.fit({\"training\" : \"s3://coco2017-34sb3-east1/coco/\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
